{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3c3c2a",
   "metadata": {},
   "source": [
    "# Popularity Prediction Project\n",
    "\n",
    "Team:\n",
    "1. Renee Dhanaraj\n",
    "2. Aditi Verma\n",
    "3. Chris Park\n",
    "4. Aryan Ahuja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17682b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, roc_auc_score, r2_score, mean_absolute_error, mean_squared_error, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54958125",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_df = pd.read_csv('dataset/dataset.csv')\n",
    "original_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7322dc",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e279bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy to clean to avoid contamination of original data\n",
    "df = original_dataset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23633b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about the dataset, such as number of entries, column names, non-null counts, and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the summary statistics of the dataset (of numerical features)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfa1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all the rows with any missing/null values since there are very few\n",
    "df = df.dropna()\n",
    "\n",
    "# IF WE PLAN TO NOT USE NLP FOR NATURAL LANGUAGE PROCESSING, UNCOMMENT AND DROP TEXT COLUMNS\n",
    "# df = df.drop(columns=[\"artists\", \"album_name\", \"track_name\", \"track_genre\"])\n",
    "\n",
    "# making sure the \"explicit\" column is of type integer and not boolean\n",
    "df[\"explicit\"] = df[\"explicit\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106e60a",
   "metadata": {},
   "source": [
    "#### Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cefde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks for the important numeric columns\n",
    "print(\"Popularity range:\", df[\"popularity\"].min(), \"to\", df[\"popularity\"].max())\n",
    "print(\"Duration range (ms):\", df[\"duration_ms\"].min(), \"to\", df[\"duration_ms\"].max())\n",
    "print(\"Tempo range:\", df[\"tempo\"].min(), \"to\", df[\"tempo\"].max())\n",
    "print(\"Loudness range:\", df[\"loudness\"].min(), \"to\", df[\"loudness\"].max())\n",
    "\n",
    "# duplicate check\n",
    "duplicates_mask = df.duplicated(subset=[\"artists\", \"album_name\", \"track_name\",\"track_id\"])\n",
    "print(\"Number of duplicate entries based on artists, album_name, track_name:\", duplicates_mask.sum())\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks for categorical columns\n",
    "print(\"Explicit values:\", df[\"explicit\"].value_counts())\n",
    "print(\"\\nMode values:\", df[\"mode\"].value_counts())\n",
    "print(\"\\nTime signature values:\", df[\"time_signature\"].value_counts())\n",
    "print(\"\\nKey values:\", df[\"key\"].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664c8a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d54466",
   "metadata": {},
   "source": [
    "#### Looking at overall distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for outliers in numerical columns using box plots\n",
    "numerical_columns = [\"popularity\", \"duration_ms\", \"tempo\", \"loudness\", \"danceability\", \"energy\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\"]\n",
    "# for col in numerical_columns:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.boxplot(df[col], vert=False)\n",
    "#     plt.title(f'Box plot of {col}')\n",
    "#     plt.xlabel(col)\n",
    "#     plt.show()\n",
    "\n",
    "# We can see that some features are skewer, but if we train models like XGBoost, for ensemble methods, we need not do any transformations\n",
    "# There are some outliers as well, but we still need to keep them since they are real songs and removing them would lead to loss of information\n",
    "# Hence, we will not be doing any outlier removal or transformations for skewness at this point\n",
    "\n",
    "# If required for training (linear models or nerual networks), we can always do log transformations or apply standardization/normalization later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['popularity'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Popularity Distribution')\n",
    "plt.xlabel('Popularity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9b2dc",
   "metadata": {},
   "source": [
    "#### Heat maps to look at relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt visualizing a headmap \n",
    "#use only numeric columns\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "#compute correlation matrix\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "#plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\" OBSERVATIONS:\n",
    "Generally a lot more negative corrolation than positive ones.\n",
    "\n",
    "Song Type\n",
    "    Negative corrolation: \n",
    "        energy + acousticness\n",
    "        loudness + acousticness\n",
    "        loudness + instrumentalness\n",
    "        valence + instrumentalness\n",
    "    \n",
    "    Positive corrolation:\n",
    "        loudness + energy\n",
    "        loudness + danceability\n",
    "        danceability + valence\n",
    "        energy + valence\n",
    "        speechiness + explicit\n",
    "\n",
    "Popularity\n",
    "    Negative corrolation:\n",
    "        popularity + instrumentalness\n",
    "        popularity + duration_ms\n",
    "\n",
    "    No positive corrolation :((\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try normalizing using log_scale to address highly skewed datasets\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, norm=mcolors.LogNorm(), cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numeric Features (Log Scaled)\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\" OBSERVATIONS:\n",
    "Generally a lot more positive corrolations\n",
    "\n",
    "Popularity\n",
    "    Negative corrolation:\n",
    "        energy\n",
    "        tempo\n",
    "\n",
    "    Positive corrolation:\n",
    "        explicit\n",
    "        danceability\n",
    "        loudness\n",
    "        time_signature\n",
    "        \n",
    "\"\"\"\n",
    "# dig deeper into the significant corrolations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240467e1",
   "metadata": {},
   "source": [
    "#### More EDA that is related to popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb393d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"popularity\"], df[\"duration_ms\"])\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Duration\")\n",
    "plt.title(\"Popularity vs duration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"popularity\"], df[\"danceability\"])\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Dancibility\")\n",
    "plt.title(\"Popularity vs Dancibility\")\n",
    "plt.show()\n",
    "\n",
    "plt.hexbin(df[\"popularity\"], df[\"danceability\"], gridsize=30, cmap='Blues')\n",
    "plt.colorbar(label='Count')\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Dancibility\")\n",
    "plt.title(\"Popularity vs Danceability (Hexbin)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df[\"popularity\"], df[\"energy\"])\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"energy\")\n",
    "plt.title(\"Popularity vs energy\")\n",
    "plt.show()\n",
    "\n",
    "plt.hexbin(df[\"popularity\"], df[\"energy\"], gridsize=30, cmap='Blues')\n",
    "plt.colorbar(label='Count')\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Dancibility\")\n",
    "plt.title(\"Popularity vs Energy (Hexbin)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f8768",
   "metadata": {},
   "source": [
    "#### 2 strong relationship features vs popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy and loudness\n",
    "df[\"energy_q\"] = pd.qcut(df[\"energy\"], 10)\n",
    "df[\"loudness_q\"] = pd.qcut(df[\"loudness\"], 10)\n",
    "pivot = df.pivot_table(values=\"popularity\", index=\"loudness_q\", columns=\"energy_q\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909641fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dancability and varience\n",
    "df[\"danceability_q\"] = pd.qcut(df[\"danceability\"], 10)\n",
    "df[\"valence_q\"] = pd.qcut(df[\"valence\"], 10)\n",
    "pivot = df.pivot_table(values=\"popularity\", index=\"danceability_q\", columns=\"valence_q\", aggfunc=\"mean\")\n",
    "sns.heatmap(pivot, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0663d3",
   "metadata": {},
   "source": [
    "#### Normalize numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece100c3-4d68-448d-be00-fa9727afe6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = df.copy()\n",
    "numerical_columns.pop(0)\n",
    "df_norm[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "df_melted = df_norm[numerical_columns].melt(var_name=\"Feature\", value_name=\"Normalized\")\n",
    "sns.boxplot(\n",
    "    data=df_melted,\n",
    "    x=\"Feature\",\n",
    "    y=\"Normalized\",\n",
    "    hue=\"Feature\",       \n",
    "    palette=\"Set2\",\n",
    "    dodge=False         \n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Normalized Boxplots for All Numerical Features\")\n",
    "plt.ylabel(\"Normalized Value (0–1)\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(df_norm[\"popularity\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b639c",
   "metadata": {},
   "source": [
    "## Feature Engineering & Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed301ee",
   "metadata": {},
   "source": [
    "### Feature Engineering / Model Training / Model Eval for Aryan's Logistic Regression Model and Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586803b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score, balanced_accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to preprocess the data\n",
    "def get_X_y(df: pd.DataFrame, \n",
    "            target_columns: List[str]):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into features and target\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=target_columns)\n",
    "    y = df[target_columns]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_popularity_classes(popularity_series: pd.Series):\n",
    "    \"\"\"\n",
    "    Converts a popularity series into discrete classes\n",
    "    \"\"\"\n",
    "    return (popularity_series >= 50).astype(int)\n",
    "\n",
    "\n",
    "def encode_data(X_train: pd.DataFrame,\n",
    "                X_val: pd.DataFrame,\n",
    "                X_test: pd.DataFrame,\n",
    "                numerical_columns: List[str],\n",
    "                feature_encode_cols: List[str],\n",
    "                onehot_encode_cols: List[str]):\n",
    "    \"\"\"\n",
    "    Encodes the data using frequency encoding, one hot encoding and standard scaling.\n",
    "    This function acts as a pipeline to fit and transform the data so you don't have to call fit and transform separately.\n",
    "    \"\"\"\n",
    "    # we need to make sure to fit the different encoders on the training data only\n",
    "    # this avoids data leakage from test and validation sets\n",
    "    frequency_map, ohe, scaler = _fit_encoders(\n",
    "        X_train, numerical_columns, feature_encode_cols, onehot_encode_cols\n",
    "    )\n",
    "\n",
    "    # once fitted, we can transform the train, validation, and test sets\n",
    "    X_train_encoded = _transform_features(\n",
    "        X_train, \n",
    "        numerical_columns, feature_encode_cols, onehot_encode_cols,\n",
    "        frequency_map, ohe, scaler\n",
    "    )\n",
    "    X_val_encoded = _transform_features(\n",
    "        X_val,\n",
    "        numerical_columns, feature_encode_cols, onehot_encode_cols,\n",
    "        frequency_map, ohe, scaler\n",
    "    )\n",
    "    X_test_encoded = _transform_features(\n",
    "        X_test, \n",
    "        numerical_columns, feature_encode_cols, onehot_encode_cols,\n",
    "        frequency_map, ohe, scaler\n",
    "    )\n",
    "\n",
    "    return X_train_encoded, X_val_encoded, X_test_encoded, frequency_map, ohe, scaler\n",
    "\n",
    "\n",
    "def _fit_encoders(X_train: pd.DataFrame,\n",
    "                 numerical_columns: List[str],\n",
    "                 feature_encode_cols: List[str],\n",
    "                 onehot_encode_cols: List[str]):\n",
    "    \"\"\"\n",
    "    PRIVATE HELPER FUNCTION\n",
    "\n",
    "    Fits the encoders for numerical and categorical features\n",
    "    \"\"\"\n",
    "    # frequency mapping for frequency encoding categorical features\n",
    "    frequency_map = {}\n",
    "    for col in feature_encode_cols:\n",
    "        frquency = X_train[col].value_counts(normalize=True) # get frequency of each category as proportion\n",
    "        frequency_map[col] = frquency.to_dict()\n",
    "\n",
    "    # one hot encoding the necessary columns\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe.fit(X_train[onehot_encode_cols])\n",
    "\n",
    "    # standard scaling the numerical columns for neural networks or linear models\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[numerical_columns])\n",
    "\n",
    "    return frequency_map, ohe, scaler\n",
    "\n",
    "\n",
    "def _transform_features(X: pd.DataFrame,\n",
    "                       numerical_columns: List[str],\n",
    "                       feature_encode_cols: List[str],\n",
    "                       onehot_encode_cols: List[str],\n",
    "                       frequency_map: dict,\n",
    "                       ohe: OneHotEncoder,\n",
    "                       scaler: StandardScaler):\n",
    "    \"\"\"\n",
    "    PRIVATE HELPER FUNCTION\n",
    "\n",
    "    Transforms the features using the fitted encoders\n",
    "    \"\"\"\n",
    "    # scaling numerical columns\n",
    "    X_std_scaled = scaler.transform(X[numerical_columns])\n",
    "\n",
    "    # transoforming frequency encoded columns\n",
    "    frequency_features = []\n",
    "    for col in feature_encode_cols:\n",
    "        frequency_map_col = frequency_map[col]\n",
    "        encoded_col = X[col].map(frequency_map_col).fillna(0).values.reshape(-1, 1)\n",
    "        frequency_features.append(encoded_col)\n",
    "    X_frequency_encoded = np.hstack(frequency_features) if frequency_features else np.empty((X.shape[0], 0))\n",
    "\n",
    "    # transforming one hot encoded columns\n",
    "    X_onehot_encoded = ohe.transform(X[onehot_encode_cols]) if onehot_encode_cols else np.empty((X.shape[0], 0))\n",
    "\n",
    "    # combining all features together\n",
    "    x = np.hstack([X_std_scaled, X_frequency_encoded, X_onehot_encoded])\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes precision, recall and balanced accuracy scores\n",
    "    \"\"\"\n",
    "    avg = \"binary\"\n",
    "    precision = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    return precision, recall, bal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaebce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will encode Artist name: Feature encoding\n",
    "# will encode Track genre: One hot encoding\n",
    "# will drop Album/track names\n",
    "# rest are numeric columns\n",
    "cols_to_drop = [\"album_name\", \"track_name\"]\n",
    "feature_encode_cols = [\"artists\"]\n",
    "onehot_encode_cols = [\"track_genre\"]\n",
    "numerical_columns = [\n",
    "    \"duration_ms\", \"explicit\", \"danceability\", \"energy\", \"key\",\n",
    "    \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\",\n",
    "    \"liveness\", \"valence\", \"tempo\", \"time_signature\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5acadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROCESSING DATA BEFORE MODEL TRAINING\n",
    "\n",
    "1. Get X and y datasets\n",
    "2. Transform popularity into classes\n",
    "3. Split into train, validation, and test sets\n",
    "4. Encode and scale the data (look at helper functions above)\n",
    "\"\"\"\n",
    "\n",
    "# getting X and y datasets\n",
    "X, y = get_X_y(df, target_columns=\"popularity\")\n",
    "\n",
    "# transforming popularity into classes\n",
    "y = make_popularity_classes(y)\n",
    "\n",
    "# splitting into train test splits\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# further split train into train and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_full, y_train_full, test_size=0.1765, stratify=y_train_full, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "UNDERSAMPLING THE MAJORITY CLASS TO BALANCE THE DATASET\n",
    "\n",
    "uncomment the below code block to perform undersampling of the majority class\n",
    "\n",
    "Either do this or in the model training phase, use class weights in cost function to balance the dataset\n",
    "\"\"\"\n",
    "# train_df = x_train.copy()\n",
    "# train_df[\"target\"] = y_train.values\n",
    "\n",
    "# # split by class\n",
    "# df_majority = train_df[train_df[\"target\"] == 0]\n",
    "# df_minority = train_df[train_df[\"target\"] == 1]\n",
    "\n",
    "# print(\"Before undersampling:\")\n",
    "# print(\"  majority (0):\", len(df_majority))\n",
    "# print(\"  minority (1):\", len(df_minority))\n",
    "\n",
    "# # downsample majority class\n",
    "# keep_factor = 0.5\n",
    "# n_majority_keep = int(len(df_majority) * keep_factor)\n",
    "\n",
    "# df_majority_down = df_majority.sample(\n",
    "#     n=n_majority_keep,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # combine and shuffle\n",
    "# df_down = pd.concat([df_majority_down, df_minority], axis=0)\n",
    "# df_down = df_down.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # new balanced-ish train X / y\n",
    "# x_train = df_down.drop(columns=[\"target\"])\n",
    "# y_train = df_down[\"target\"].values\n",
    "\n",
    "# print(\"After undersampling:\")\n",
    "# print(\"  majority (0):\", (y_train == 0).sum())\n",
    "# print(\"  minority (1):\", (y_train == 1).sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# encoding and scaling the data\n",
    "X_train_encoded, X_val_encoded, X_test_encoded, frequency_map, ohe, scaler = encode_data(\n",
    "    x_train, x_val, x_test,\n",
    "    numerical_columns,\n",
    "    feature_encode_cols,\n",
    "    onehot_encode_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEFINING A BASE MODEL TO COMPARE MY MODEL AGAINST\n",
    "\n",
    "BASE MODEL: LOGISTIC REGRESSION\n",
    "\"\"\"\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    r2_score,\n",
    "    mean_squared_error\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=5000,\n",
    "    multi_class=\"auto\",\n",
    "    solver=\"lbfgs\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# starting grid search\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "\n",
    "# post training\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "\n",
    "# validation set evaluation\n",
    "y_val_pred = best_lr_model.predict(X_val_encoded)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred, average=\"binary\")\n",
    "\n",
    "# including r2 and rmse for... classification metrics?\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf13643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# making a dataset class\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = np.asarray(X, dtype=np.float32)\n",
    "        self.y = np.asarray(y, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y\n",
    "\n",
    "# create datasets from your encoded splits\n",
    "train_ds = SongDataset(X_train_encoded, y_train)\n",
    "val_ds   = SongDataset(X_val_encoded,   y_val)\n",
    "test_ds  = SongDataset(X_test_encoded,  y_test)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# defining the model\n",
    "input_dim  = X_train_encoded.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "class PopularityMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer11 = nn.Linear(input_dim, 4096)\n",
    "        self.batch_norm11   = nn.BatchNorm1d(4096)\n",
    "        self.relu11         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer12 = nn.Linear(4096, 2048)\n",
    "        self.batch_norm12   = nn.BatchNorm1d(2048)\n",
    "        self.relu12         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer0 = nn.Linear(2048, 1024)\n",
    "        self.batch_norm0   = nn.BatchNorm1d(1024)\n",
    "        self.relu0         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(1024, 512)\n",
    "        self.batch_norm1   = nn.BatchNorm1d(512)\n",
    "        self.relu1         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer2 = nn.Linear(512, 256)\n",
    "        self.batch_norm2   = nn.BatchNorm1d(256)\n",
    "        self.relu2         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer3 = nn.Linear(256, 128)\n",
    "        self.batch_norm3   = nn.BatchNorm1d(128)\n",
    "        self.relu3         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer4 = nn.Linear(128, 64)\n",
    "        self.batch_norm4   = nn.BatchNorm1d(64)\n",
    "        self.relu4         = nn.LeakyReLU()\n",
    "\n",
    "        self.linear_layer5 = nn.Linear(64, 32)\n",
    "        self.batch_norm5   = nn.BatchNorm1d(32)\n",
    "        self.relu5         = nn.LeakyReLU()\n",
    "\n",
    "        self.out = nn.Linear(64, output_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear_layer11(x)\n",
    "        x = self.batch_norm11(x)\n",
    "        x = self.relu11(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.linear_layer12(x)\n",
    "        x = self.batch_norm12(x)\n",
    "        x = self.relu12(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.linear_layer0(x)\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = nn.Dropout(0.4)(x)\n",
    "\n",
    "        x = self.linear_layer1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = nn.Dropout(0.4)(x)\n",
    "\n",
    "        x = self.linear_layer2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.linear_layer3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.linear_layer4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # x = self.linear_layer5(x)\n",
    "        # x = self.batch_norm5(x)\n",
    "        # x = self.relu5(x)\n",
    "\n",
    "        # output (no activation here for BCEWithLogitsLoss)\n",
    "        return self.out(x)\n",
    "\n",
    "model = PopularityMLP().to(device)\n",
    "\n",
    "\n",
    "pos_ratio = y_train.mean()\n",
    "neg_ratio = 1 - pos_ratio\n",
    "pos_weight = torch.tensor(neg_ratio / pos_ratio, device=device)\n",
    "\n",
    "# loss function, optimizer, scheduler\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-3,\n",
    "    weight_decay=1e-3,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "# epoch runner\n",
    "def run_epoch(loader, model, criterion, optimizer=None, scheduler=None):\n",
    "    \"\"\"\n",
    "    If optimizer is provided -> training mode\n",
    "    If optimizer is None -> evaluation mode\n",
    "    \"\"\"\n",
    "    if optimizer is None:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "    running_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(optimizer is not None):\n",
    "            logits = model(xb)\n",
    "\n",
    "            yb_float = yb.float().unsqueeze(1)\n",
    "            loss = criterion(logits, yb_float)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).long().squeeze(1)\n",
    "\n",
    "        if optimizer is not None:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        all_labels.append(yb.detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "\n",
    "    avg_loss = running_loss / total_batches\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_preds  = torch.cat(all_preds).numpy()\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"binary\")\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, acc, f1, all_labels, all_preds\n",
    "\n",
    "\n",
    "# training loop count\n",
    "EPOCHS = 50\n",
    "\n",
    "USE_EARLY_STOPPING = True   # toggle this True/False\n",
    "PATIENCE = 20       # how many epochs with no val loss improvement\n",
    "MIN_DELTA = 0.0     # minimum improvement in loss to reset patience\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc, train_f1, _, _ = run_epoch(\n",
    "        train_loader, model, criterion, optimizer, scheduler\n",
    "    )\n",
    "    val_loss, val_acc, val_f1, _, _ = run_epoch(\n",
    "        val_loader, model, criterion, optimizer=None\n",
    "    )\n",
    "    \n",
    "    # Step scheduler once per epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- Early stopping and best-model tracking based on val_loss ----\n",
    "    # We always track the best model by lowest val_loss\n",
    "    if val_loss + MIN_DELTA < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        if USE_EARLY_STOPPING:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch} (no val_loss improvement)\")\n",
    "                break\n",
    "\n",
    "    # Logging every 25 epochs (you can change this)\n",
    "    if epoch % 25 == 0 or epoch == EPOCHS:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} f1={train_f1:.4f} | \"\n",
    "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} f1={val_f1:.4f} | \"\n",
    "            f\"lr={optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "\n",
    "# loading best model     \n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nLoaded best model with val_loss = {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final evaluation on train, val, and test sets\n",
    "\n",
    "train_loss, train_acc, train_f1, y_train_true, y_train_pred = run_epoch(\n",
    "    train_loader, model, criterion, optimizer=None\n",
    ")\n",
    "val_loss, val_acc, val_f1, y_val_true, y_val_pred = run_epoch(\n",
    "    val_loader, model, criterion, optimizer=None\n",
    ")\n",
    "test_loss, test_acc, test_f1, y_test_true, y_test_pred = run_epoch(\n",
    "    test_loader, model, criterion, optimizer=None\n",
    ")\n",
    "\n",
    "train_prec, train_rec, train_bal_acc = compute_classification_metrics(\n",
    "    y_train_true, y_train_pred\n",
    ")\n",
    "val_prec, val_rec, val_bal_acc = compute_classification_metrics(\n",
    "    y_val_true, y_val_pred\n",
    ")\n",
    "test_prec, test_rec, test_bal_acc = compute_classification_metrics(\n",
    "    y_test_true, y_test_pred\n",
    ")\n",
    "\n",
    "print(\"\\nFinal train/test/val metrics which include precision, recall, balanced accuracy\")\n",
    "print(f\"Train: loss={train_loss:.4f} acc={train_acc:.4f} f1={train_f1:.4f} \"\n",
    "      f\"prec={train_prec:.4f} rec={train_rec:.4f} bal_acc={train_bal_acc:.4f} \")\n",
    "print(f\"Val:   loss={val_loss:.4f} acc={val_acc:.4f} f1={val_f1:.4f} \"\n",
    "      f\"prec={val_prec:.4f} rec={val_rec:.4f} bal_acc={val_bal_acc:.4f} \")\n",
    "print(f\"Test:  loss={test_loss:.4f} acc={test_acc:.4f} f1={test_f1:.4f} \"\n",
    "      f\"prec={test_prec:.4f} rec={test_rec:.4f} bal_acc={test_bal_acc:.4f} \")\n",
    "\n",
    "print(\"\\npytorch MLP test performance\")\n",
    "print(f\"test loss: {test_loss:.4f}\")\n",
    "print(f\"test accuracy: {test_acc:.4f}\")\n",
    "print(f\"test f1 (binary): {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nclassification report (test dataset):\")\n",
    "print(classification_report(y_test_true, y_test_pred, digits=4))\n",
    "\n",
    "print(\"confusion matrix (test dataset):\")\n",
    "print(confusion_matrix(y_test_true, y_test_pred))\n",
    "\n",
    "# printing out test accuracy and area under curve\n",
    "model.eval()\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "\n",
    "print(\"=== Test ===\")\n",
    "test_probs = np.concatenate(all_probs)\n",
    "print(f\"accuracy: {test_acc}\")\n",
    "test_auc = roc_auc_score(y_test_true, test_probs)\n",
    "print(f\"AUC: {test_auc}\")\n",
    "\n",
    "# printing chart for confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bdfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae7c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6585d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d7b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7926f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5b20f9",
   "metadata": {},
   "source": [
    "#### divide data into popular and non popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5548c079-672e-4fac-a947-9c776508a41f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m    0: non popular song\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    1: popular song\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf\u001b[49m.loc[df[\u001b[33m'\u001b[39m\u001b[33mpopularity\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m50\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpopularity\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m0\u001b[39m   \u001b[38;5;66;03m# Low\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# df_norm.loc[(df_norm['popularity'] >= 30) & (df_norm['popularity'] < 40), 'popularity'] = 1  # Medium\u001b[39;00m\n\u001b[32m      7\u001b[39m df.loc[df[\u001b[33m'\u001b[39m\u001b[33mpopularity\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[32m50\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpopularity\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# High\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    0: non popular song\n",
    "    1: popular song\n",
    "\"\"\"\n",
    "df.loc[df['popularity'] < 50, 'popularity'] = 0   # Low\n",
    "# df_norm.loc[(df_norm['popularity'] >= 30) & (df_norm['popularity'] < 40), 'popularity'] = 1  # Medium\n",
    "df.loc[df['popularity'] >= 50, 'popularity'] = 1  # High\n",
    "\n",
    "df['popularity'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a00e7a",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95190d8a-5188-4e81-beb1-2ce0aa222037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature(df, col):\n",
    "    X = df[col].copy()\n",
    "    y = df[\"popularity\"].astype(int)\n",
    "    return X, y\n",
    "\n",
    "def precision_at_k(predictions, y_true, ks=[1, 100, 1000, 10000]):\n",
    "    precs = []\n",
    "    for K in ks:\n",
    "        top_pred = predictions[:min(K, len(predictions))]\n",
    "        # true positives: ground truth values at top predicted indices\n",
    "        true_positives = np.sum(y_true[top_pred] == 1)\n",
    "        precs.append(true_positives / len(top_pred))\n",
    "    return precs\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def evaluate(model, X, y, name=\"set\"):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_class = np.round(y_pred).astype(int)\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"accuarcy:\", accuracy_score(y,y_pred_class))\n",
    "    print(\"R²:\", r2_score(y, y_pred))\n",
    "    print(\"MAE:\", mean_absolute_error(y, y_pred))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y, y_pred)))\n",
    "    print(\"MSE:\", mse(y, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate_classification(model, X, y, name=\"set\", threshold = 0.5):\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(\"accuarcy:\", accuracy_score(y,y_pred))\n",
    "    print(\"AUC:\", roc_auc_score(y,y_pred))\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13570f9",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdca52-cb63-4129-9314-d20d90d3fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(0)       \n",
    "df_shuffled = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "N = len(df)\n",
    "\n",
    "train_end = int(0.8 * N)\n",
    "valid_end = int(0.9 * N)   # 50% + 25%\n",
    "\n",
    "dataTrain = df_shuffled[:train_end]\n",
    "dataValid = df_shuffled[train_end:valid_end]\n",
    "dataTest  = df_shuffled[valid_end:]\n",
    "\n",
    "dfTrain = pd.DataFrame(dataTrain)\n",
    "dfValid = pd.DataFrame(dataValid)\n",
    "dfTest  = pd.DataFrame(dataTest)\n",
    "\n",
    "\"\"\"FROM LOG-SCALED HEATMAP\n",
    "    explicit          binary (0-1)\n",
    "    danceability      float64\n",
    "    loudness          float64\n",
    "    time_signature    int64 (0-4)\n",
    "    energy            float64\n",
    "    tempo             float64\n",
    "\n",
    "    key               int64 (0-11) \n",
    "\"\"\"\n",
    "#features we got from heatmap\n",
    "heatmap_feat = [\n",
    "    \"explicit\",\n",
    "    \"danceability\",\n",
    "    \"loudness\",\n",
    "    \"tempo\",\n",
    "    \"energy\",\n",
    "    \"time_signature\",\n",
    "    \"speechiness\",\n",
    "    \"liveness\",\n",
    "    \"key\",\n",
    "    \"acousticness\",\n",
    "    \"valence\"\n",
    "]\n",
    "\n",
    "all_feat = [\n",
    "    \"explicit\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"loudness\",\n",
    "    \"tempo\",\n",
    "    \"acousticness\",\n",
    "    \"speechiness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"duration_ms\",\n",
    "    \"key\",\n",
    "    \"mode\",\n",
    "    \"time_signature\"\n",
    "]\n",
    "\n",
    "X_train, y_train = feature(dfTrain, heatmap_feat) #50%\n",
    "X_valid, y_valid = feature(dfValid, heatmap_feat) #25%\n",
    "X_test,  y_test  = feature(dfTest, heatmap_feat)  #25%\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(\"Before SMOTE:\")\n",
    "# print(y_train.value_counts())\n",
    "\n",
    "# print(\"\\nAfter SMOTE:\")\n",
    "# print(y_train_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f133f",
   "metadata": {},
   "source": [
    "#### Scale variables using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "# X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719355e8",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "evaluate_classification(logistic_model, X_train_scaled, y_train, threshold=0.5)\n",
    "evaluate_classification(logistic_model, X_valid_scaled, y_valid, \"validation\", threshold=0.5)\n",
    "evaluate_classification(logistic_model, X_test_scaled, y_test, \"test\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4b2cf",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb284095",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "evaluate_classification(xgb_model, X_train, y_train, \"train\", threshold=0.5)\n",
    "evaluate_classification(xgb_model, X_valid, y_valid, \"Validation\", threshold=0.5)\n",
    "evaluate_classification(xgb_model, X_test, y_test, \"Test\", threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aba271",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dad324",
   "metadata": {},
   "source": [
    "##### Using GridSearch for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73998ad-9915-423d-8344-c4a86ba51981",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "params_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [5, 10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid= RandomizedSearchCV(\n",
    "    rf_model,\n",
    "    params_grid,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring= \"accuracy\",\n",
    "    n_jobs=1,\n",
    "    verbose = 2,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50256163",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification(rf_model, X_train, y_train, \"train\", threshold = 0.5)\n",
    "evaluate_classification(rf_model, X_valid, y_valid, \"valid\", threshold = 0.5)\n",
    "evaluate_classification(rf_model, X_test,  y_test,  \"test\", threshold = 0.5)\n",
    "print(rf_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665df096",
   "metadata": {},
   "source": [
    "#### Using Random Forest to get important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac1c22",
   "metadata": {},
   "source": [
    "#### Cross validation to see if we are overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    rf_model, \n",
    "    X_train,    \n",
    "    y_train, \n",
    "    cv=20,\n",
    "    scoring='f1'\n",
    ")\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean CV score:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b654cd",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
